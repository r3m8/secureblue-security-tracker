{
  "description": "In the Linux kernel, the following vulnerability has been resolved:  rxrpc: Fix locking in rxrpc's sendmsg  Fix three bugs in the rxrpc's sendmsg implementation:   (1) rxrpc_new_client_call() should release the socket lock when returning      an error from rxrpc_get_call_slot().   (2) rxrpc_wait_for_tx_window_intr() will return without the call mutex      held in the event that we're interrupted by a signal whilst waiting      for tx space on the socket or relocking the call mutex afterwards.       Fix this by: (a) moving the unlock/lock of the call mutex up to      rxrpc_send_data() such that the lock is not held around all of      rxrpc_wait_for_tx_window*() and (b) indicating to higher callers      whether we're return with the lock dropped.  Note that this means      recvmsg() will not block on this call whilst we're waiting.   (3) After dropping and regaining the call mutex, rxrpc_send_data() needs      to go and recheck the state of the tx_pending buffer and the      tx_total_len check in case we raced with another sendmsg() on the same      call.  Thinking on this some more, it might make sense to have different locks for sendmsg() and recvmsg().  There's probably no need to make recvmsg() wait for sendmsg().  It does mean that recvmsg() can return MSG_EOR indicating that a call is dead before a sendmsg() to that call returns - but that can currently happen anyway.  Without fix (2), something like the following can be induced:  \tWARNING: bad unlock balance detected! \t5.16.0-rc6-syzkaller #0 Not tainted \t------------------------------------- \tsyz-executor011/3597 is trying to release lock (&call->user_mutex) at: \t[<ffffffff885163a3>] rxrpc_do_sendmsg+0xc13/0x1350 net/rxrpc/sendmsg.c:748 \tbut there are no more locks to release!  \tother info that might help us debug this: \tno locks held by syz-executor011/3597. \t... \tCall Trace: \t <TASK> \t __dump_stack lib/dump_stack.c:88 [inline] \t dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106 \t print_unlock_imbalance_bug include/trace/events/lock.h:58 [inline] \t __lock_release kernel/locking/lockdep.c:5306 [inline] \t lock_release.cold+0x49/0x4e kernel/locking/lockdep.c:5657 \t __mutex_unlock_slowpath+0x99/0x5e0 kernel/locking/mutex.c:900 \t rxrpc_do_sendmsg+0xc13/0x1350 net/rxrpc/sendmsg.c:748 \t rxrpc_sendmsg+0x420/0x630 net/rxrpc/af_rxrpc.c:561 \t sock_sendmsg_nosec net/socket.c:704 [inline] \t sock_sendmsg+0xcf/0x120 net/socket.c:724 \t ____sys_sendmsg+0x6e8/0x810 net/socket.c:2409 \t ___sys_sendmsg+0xf3/0x170 net/socket.c:2463 \t __sys_sendmsg+0xe5/0x1b0 net/socket.c:2492 \t do_syscall_x64 arch/x86/entry/common.c:50 [inline] \t do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80 \t entry_SYSCALL_64_after_hwframe+0x44/0xae  [Thanks to Hawkins Jiawei and Khalid Masum for their attempts to fix this]",
  "scope": "local",
  "releases": {
    "bookworm": {
      "status": "resolved",
      "repositories": {
        "bookworm": "6.1.148-1",
        "bookworm-security": "6.1.158-1"
      },
      "fixed_version": "6.0.2-1",
      "urgency": "not yet assigned"
    },
    "bullseye": {
      "status": "resolved",
      "repositories": {
        "bullseye": "5.10.223-1",
        "bullseye-security": "5.10.244-1"
      },
      "fixed_version": "5.10.140-1",
      "urgency": "not yet assigned"
    },
    "forky": {
      "status": "resolved",
      "repositories": {
        "forky": "6.16.12-2"
      },
      "fixed_version": "6.0.2-1",
      "urgency": "not yet assigned"
    },
    "sid": {
      "status": "resolved",
      "repositories": {
        "sid": "6.17.7-2"
      },
      "fixed_version": "6.0.2-1",
      "urgency": "not yet assigned"
    },
    "trixie": {
      "status": "resolved",
      "repositories": {
        "trixie": "6.12.43-1",
        "trixie-security": "6.12.48-1"
      },
      "fixed_version": "6.0.2-1",
      "urgency": "not yet assigned"
    }
  }
}